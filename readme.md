
<h1 align="center">
    <a href="https://github.com/girish-lab/Safe-Segmentation-Competition-ICPR-24"> ICPR 2024 Competition on Safe Segmentation of Drive Scenes in  Unstructured Traffic and Adverse Weather Conditions</a>
</h1>

<p align="center">
is a competition running for several months aimed at assessing the performance of semantic segmentation models in adverse weather conditions for autonomous driving. Participants are provided with the IDD-AW dataset, comprising 5000 pairs of high-quality images with pixel-level annotations, captured under adverse weather conditions such as rain, fog, low light, and snow.
</p>
    
<h2 align="center"> Updates </h2>

        1. Regristration Open.
        2. 
        3. 

<h2 align="center"> Regrestration </h2>


<h2 align="center"> Objective </h2>

<p align="center">
The central challenge addressed by the competition is optimizing the Safe mIoU metric for semantic segmentation in adverse weather conditions to ensure the safety of autonomous driving systems. Semantic segmentation plays a pivotal role in enabling autonomous vehicles to understand their surroundings, but adverse weather conditions pose significant challenges, often leading to hazardous situations. The competition aims to develop robust models that accurately segment driving scenes even in adverse weather conditions, focusing on prioritizing safety-related mispredictions.
</p>

<h2 align="center"> Timeline </h2>

    1. Reg. Deadline: July 21, 2024
    2. Competition Report Submission Deadline: August 18, 2024
    3. Camera-Ready Papers: September 02, 2024	
    4. Communicate Winners to Chairs: October 28, 2024	
    5. Presentation at ICPR Conference: December 1-5, 2024	

<h2 align="center"> Guidlines </h2>

<h2 align="center"> Awards </h2>

    1 
    2
    3



<h2 align="center"> Dataset details </h2>



<p align="center">
The dataset consists of images collected in an unstructured road scenario, driving in adverse weather conditions of rain, fog, lowlight and snow. Each individual RGB image has a more detailed near-infrared image (NIR) captured simultaneously. The images are collected using JAI FS-3200D-10GE camera.
</p>


<a align="center" href="https://iddaw.github.io/"> Dataset page</a>

<a align="center" href="https://idd.insaan.iiit.ac.in/dataset/download/">Download here</a>

<p align="center">
The dataset, including training data, ground truth, and an evaluation script, will be made publicly available post-competition in accordance with ICPR guidelines.
</p>

<h2 align="center"> Methods: </h2>


<h2 align="center"> Evaluation Matrics: </h2>


    1 SafeIoU
    2 ...
    3. code [link]([https:](https://github.com/Furqan7007/IDDAW_kit))

<h2 align="center"> Leaderboard </h2>


<h2 align="center"> Organigers </h2>
    
Dr Girish Varma, C-STAR and ML Lab at IIIT Hyderabad

Furqan Ahamad, ML Lab, IIIT-Hyderabad

Sandeep Nagar, ML Lab, IIIT-Hyderabad





<h2 align="center"> Refrence </h2>

https://iddaw.github.io/

Furqan Ahmed Shaik, Abhishek Reddy, Nikhil Reddy Billa, Kunal Chaudhary, Sunny Manchanda, Girish Varma; Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2024, pp. 4614-4623

<h2 align="center">
    FAQ:
</h2>

    1
    2
    3
    4
    5
    6
    

<h2 align="center">
    Contacts:
</h2>
If you have any question regarding to this Competition, please send an email to the following address:

furqan.shaik@research.iiit.ac.in or sandeep.nagar@research.iiit.ac.in

Furqan Ahamad

[Sandeep Nagar](https://twitter.com/NaagarRN)

[girish-lab](https://girishvarma.in/)

