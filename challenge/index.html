<!DOCTYPE HTML>

<html>
	
<!-- Mirrored from cvit.iiit.ac.in/autonue2021/challenge/ by HTTrack Website Copier/3.x [XR&CO'2014], Thu, 25 Apr 2024 11:44:10 GMT -->
<head>
		<title>AutoNUE 2021 Challenge</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="../assets/css/main.css" />
		<style>
			.updates{
				background-color: #6f42c1;

			}
		</style>
	</head>
	<body>

		<!-- Header -->
			<header id="header">
				<div class="inner">
					<!-- <a href="index.html" class="logo">AutoNUE 2021</a> -->
					<a href="index-2.html" class="logo"><img src="../images/autonue-logo-cvpr.png" width="250"></a>
					<nav id="nav">
						<a href="index-2.html">Home</a>
						<a href="overview.html">Overview</a>
						<a href="benchmark.html">Instructions & Benchmark </a>
                        <a href="../index.html" target="_blank">AutoNUE 2021 </a>
						<a href="faq.html">FAQ</a>
						<a href="contact.html">Contact US</a>
					</nav>
				</div>
			</header>
			<a href="#menu" class="navPanelToggle"><span class="fa fa-bars"></span></a>

		<!-- Banner -->
			<section id="banner" style="background-image: url('img/challenge-banner.jpg');">
				<div class="inner">
                    <h1>
                        Scene Understanding Challenge for Autonomous Navigation in Unstructured Environments <br>
                    
                        In Conjunction with <a href="http://cvpr2021.thecvf.com/"> CVPR 2021</a>, June 19, 2021
                    </h1>
					<!-- <ul class="actions">
						<li><a href="#" class="button alt">Get Started</a></li>
					</ul> -->
				</div>
			</section>

		<!-- One -->

        <!-- One -->
			<section id="one">
				<div class="inner">
					<header>
						<h2>Challenge </h2>
                    </header>


					<p>
                        While several datasets for autonomous navigation have become available in recent years, they have focused on structured driving 
                        environments. This usually corresponds to well-delineated infrastructures such as lanes, a small number of well-defined categories
                         for traffic participants, low variation in the object or background appearance, and firm adherence to traffic rules. Under the 
                         purview of domain adaptation, the community has also studied various aspects such as weather changes, time of day, or imaging 
                         conditions. We propose the novel dataset for segmentation and domain adaptation in unstructured environments where the above
                          assumptions are mostly unsatisfied. 
                        </p>
                        <p>
                            The challenge will feature 
                            <ul>
                                
								<!-- <li>IDD Dataset for segmentation and domain adaptation.</li> -->
								<li> Challenges for domain adaptation with varying levels of supervision.</li>
                                <li> Challenges for semantic segmentation.</li>
                               
                                
                            </ul>

							If you are interested in participating, please register here: <a href="https://docs.google.com/forms/d/1SNIiIMvDR2LNLyLAY_hCPY2Hv4pfzsHdJZ052mjF5Cc/edit">Click Here</a>
                       
						</p>


						<header>
                            <h2>Challenge Details  </h2>
                        </header>

						<p>
							<ul>
								<li>
									Challenge Announcement : 
									<strong>January 10, 2021 (IST)</strong>
								   </li>
									
								   <li>
								 Data available for Download :
									 <strong> February 10, 2021 (IST)</strong>
								   
								   </li>
								  
							  <li>
                                                                       
                                                 Challenge Submission Deadline :
                                                 <strike> <strong>June 5, 2021 (IST)</strong> </strike>
                                                                        <strong> June 13, 2021 (11:59 p.m. IST) </strong>
                                                   </li>


                                                   <li>
                                                                                Winner Announcement:
                                                        <strike> <strong> June 7, 2021 </strong> 
                                                        <strong> June 15, 2021 (11:59 p.m. IST) </strong> </strike> Will soon update.
                                                   </li>
						
							   </ul>
						</p>


						<header>
                            <h2>Prizes  </h2>
                        </header>

						<p>
							Intel is sponsoring prize money of 1000USD for the winners of each of the 5 challenges. 
						</p>


						<header>
                            <h2>Challenge Winners  </h2>
                        </header>

						<p>
							<strong>1)  Semantic Segmentation  </strong> <br> 
							<strong> (i) Challenge Winner Team Name : PaddleSeg </strong> <br>
							<strong> Team Members </strong> 

							<div class="row">
								<div class="3u 6u(small) 12u$(xsmall)">
								   <img src="img/winners/Li.jpg" alt="LI" /> 
							 </div> 

							 <div class="3u 6u(small) 12u$(xsmall)">
								<img src="img/winners/tianfei_zhou.jpg" alt="LI" /> 
						  </div> 

						  <div class="3u 6u(small) 12u$(xsmall)">
							<img src="img/winners/wenguan_wang.jpg" alt="LI" /> 
					  </div> 

<div class="3u 6u(small) 12u$(xsmall)">
								   <img src="img/winners/Yi.jpg" alt="LI" /> 
							 </div> 
					

									  
							</div>



							<div class="row">
								

							 <div class="3u 6u(small) 12u$(xsmall)">
								<img src="img/winners/Yu.jpg" alt="LI" /> 
						  </div> 

						  <div class="3u 6u(small) 12u$(xsmall)">
							<img src="img/winners/Zeyu.jpg" alt="LI" /> 
					  </div> 


									  
							</div>
							

							<ol>
								<li> Liulei Li,  <i>  Beijing Institute of Technology & Baidu, Inc.</i> </li>
								<li>  Tianfei Zhou,   <i> Computer Vision Lab, ETH Zürich</i> </li>
								<li>  Yi Liu, <i>  Baidu, Inc.</i> </li>
								<li> Lu Yang,   <i>  Lab of Pattern Recognition and Intelligent Vision, Beijing University of Posts and Telecommunications </i>  </li>
								<li> Zeyu Chen,   <i>  Baidu, Inc </i></li>
								<li> Wenguan Wang,  <i>  Computer Vision Lab, ETH Zürich </i> </li>
							</ol>



							<strong> (ii) Runner UP Team Name : Tencent Youtu Lab </strong> <br>
							<strong> Team Members </strong> 
							<ol>
								<li> Ting Sun, </li>
								<li> Shuyi Zhang </li>
								<li> Yijun Feng </li>
								<li> Yong Liu </li>
							</ol>


						</p>

						<p>
								<strong> 2) Supervised Domain Adaptation Challenge </strong> <br>
								<strong>Challenge Winner Team Name: Tencent Youtu Lab </strong> <br>
								<strong> Team Members </strong>


								<div class="row">
									<div class="3u 6u(small) 12u$(xsmall)">
									   <img src="img/winners/01.jpg" alt="LI" /> 
								 </div> 
	
								 <div class="3u 6u(small) 12u$(xsmall)">
									<img src="img/winners/02.jpg" alt="LI" /> 
							  </div> 
	
							  <div class="3u 6u(small) 12u$(xsmall)">
								<img src="img/winners/04.jpg" alt="LI" /> 
						  </div> 
	
						  <div class="3u 6u(small) 12u$(xsmall)">
							<img src="img/winners/03.jpg" alt="LI" /> 
					  </div> 
										  
								</div>


								
								<ol>
									<li> Ting Sun, </li>
									<li> Shuyi Zhang </li>
									<li> Yijun Feng </li>
									<li> Yong Liu </li>
								</ol>
						</p>


						<p>
							<strong> 3) Semi-Supervised Domain Adaptation Challenge  </strong> <br>
							<strong>Challenge Winner Team Name: Tencent Youtu Lab </strong> <br>
							<strong> Team Members </strong>

							<ol>
								<li> Ting Sun, </li>
								<li> Shuyi Zhang </li>
								<li> Yijun Feng </li>
								<li> Yong Liu </li>
							</ol>
					</p>



					<p>
						<strong> 4) Unsupervised Domain Adaptation Challenge  </strong> <br>
						<strong>Challenge Winner Team Name: Tencent Youtu Lab </strong> <br>
						<strong> Team Members </strong>

						<ol>
							<li> Ting Sun, </li>
							<li> Shuyi Zhang </li>
							<li> Yijun Feng </li>
							<li> Yong Liu </li>
						</ol>
				</p>


				<p>
					<strong> 5) Weakly Supervised Domain Adaptation Challenge  </strong> <br>
					<strong>Challenge Winner Team Name: Tencent Youtu Lab </strong> <br>
					<strong> Team Members </strong>

					<ol>
						<li> Ting Sun, </li>
						<li> Shuyi Zhang </li>
						<li> Yijun Feng </li>
						<li> Yong Liu </li>
					</ol>
			</p>
						
                        <header>
                            <h2>Challenge Details  </h2>
                        </header>
                        <p>


							<br>

							<strong>I. Domain Adaptation Challenge: </strong> <br>

							Manually annotating new data is effort-intensive and not the ideal solution we would like to rely on each time we want to fine-tune a model to a specific location. We intend to tackle this issue in this challenge - especially in adapting models to the challenging IDD domain from multiple source domains.
						 For the source dataset (S), we introduce significant diversity in different dimensions: <a href="https://www.mapillary.com/"> Mapillary</a>, <a href="https://www.cityscapes-dataset.com/"> Cityscapes</a>, <a href="https://bdd-data.berkeley.edu/">Berkeley Deep Drive</a>, and <a href="https://download.visinf.tu-darmstadt.de/data/from_games/"> GTA</a>. We sample around 20,000 images from these four datasets depending on several factors (e.g., number of classes and inherent complexity). The source dataset remains uniform for all the sub-tasks.
						 We consider level 2 (16 classes) and level 3 (26 classes) ids of IDD for label spaces in target datasets (T), which provide closed and open-set domain adaptation opportunities (refer to Figure 2 and Section 3.3 of <a href="https://idd.insaan.iiit.ac.in/">IDD</a> for more details of levels of hierarchy in class labels). The target dataset changes for each sub-task. 
						 
						 <br> <br>



						 <strong>II. Segmentation Challenge: </strong> <br>
						 Results from past challenges have highlighted the relative difficulty of the IDD dataset. The dataset used for previous challenges consisted of 20,000 images, finely annotated with 34 classes collected over 200 drive sequences on Indian roads. The label set was expanded compared to popular benchmarks such as Cityscapes, accounting for the new classes.

This task involves semantic segmentation, which are now widespread in the computer vision community. 

</p>

In total, we host following four Domain Adaptation challenges and a semantic segmentation challenge:

							<ol>
								<li>Supervised Domain Adaptation: Data available is S + T.</li>
								<li>Semi-supervised Domain Adaptation: Data available is  S+ fractions of T</li>
								<li>Weakly supervised: Data available is  S + T ′ (Labels like image-level labels and bounding boxes for IDD)</li>
								<li>Unsupervised Domain Adaptation: Data available is S only.</li>
								<li>Semantic Segmentation: Data available is T only.</li>
							  </ol>
							

					<p>
						For more details, refer <a href="benchmark.html"> instructions and benchmark </a>
					</p>		



<p>
	For any queries regarding the dataset or the challenge, feel free to drop a mail to :  <a href="mailto:idd.challenge@gmail.com"> idd.challenge@gmail.com</a> ,
	<!-- <br>  Datasets can be downloaded as per the Readme and csv files given at <a href="https://github.com/AutoNUE/AutoNUE2021_DomainAdaptationChallenge">https://github.com/AutoNUE/AutoNUE2021_DomainAdaptationChallenge</a> -->
<br>
Refer to the <a href="../faq.html">FAQ</a> where we have replied to some of the common queries.
</p>


<header>
	<h2>Organizers </h2>
</header>

<div class="box alt">
	<div class="row 50% uniform">
		
		<div class="2u"><span class="image fit"><img src="img/iiit-new.png" alt="iiith"/></span></div>
		<div class="2u"><span class="image fit"><img src="img/intel-logo-challenge.jpg" alt="intel" /></span></div>
		<!-- <div class="2u"><span class="image fit"><img src="img/University_of_Maryland_seal.svg.png" alt="intel" /></span></div> -->
		<div class="2u"><span class="image fit"><img style="margin-top: 30px;" src="img/us.jpg" alt="UC" /></span></div>
		<div class="2u"><span class="image fit"><img style="margin-top: 30px;" src="img/iith.png" alt="iiith"  /></span></div>
		
		
	</div>
</div>


<header>
    <h2>References  </h2>
</header>
<p>
<ul>
    <li>Neuhold, Gerhard, Tobias Ollmann, Samuel Rota Bulo, and Peter Kontschieder. <a href="../../../openaccess.thecvf.com/content_ICCV_2017/papers/Neuhold_The_Mapillary_Vistas_ICCV_2017_paper.pdf"> The mapillary vistas dataset for semantic understanding of street scenes</a>. In Proc.  IEEE International Conference on Computer Vision, (pp. 4990-4999) Venice, Italy, Oct 2017.</li>
<li>Cordts, Marius, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. <a href="../../../openaccess.thecvf.com/content_cvpr_2016/papers/Cordts_The_Cityscapes_Dataset_CVPR_2016_paper.pdf">The cityscapes dataset for semantic urban scene understanding</a>. In Proc.  IEEE/CVF Conference on Computer Vision and Pattern Recognition, (pp. 3213-3223) Las Vegas, US, Jun 2016.</li>
<li>Yu, Fisher, Haofeng Chen, Xin Wang, Wenqi Xian, Yingying Chen, Fangchen Liu, Vashisht Madhavan, and Trevor Darrell. <a href="../../../openaccess.thecvf.com/content_CVPR_2020/papers/Yu_BDD100K_A_Diverse_Driving_Dataset_for_Heterogeneous_Multitask_Learning_CVPR_2020_paper.pdf">BDD100K: A diverse driving dataset for heterogeneous multitask learning</a>. In Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition, (pp. 2636-2645), June 2020.</li>
<li>Richter, Stephan R., Vibhav Vineet, Stefan Roth, and Vladlen Koltun. <a href="../../../arxiv.org/pdf/1608.02192.pdf">Playing for data: Ground truth from computer games</a>. In European Conference on Computer Vision, (pp. 102-118), Springer, Cham, 2016.</li>
<li>Girish Varma, Anbumani Subramanian, Anoop Namboodiri, Manmohan Chandraker, and C.V. Jawahar. <a href="../../images/ConferencePapers/2019/idd_autonomous_navigation.pdf">IDD: A dataset for exploring problems of autonomous navigation in unconstrained environments</a>. In Proc. IEEE Winter Conference on Applications of Computer Vision, Hawaii, USA, Jan 2019.</li>
</ul>
</p>


				</div>
			</section>


	

		<!-- Footer -->
			<section id="footer">
				<div class="inner">
							
					<div class="copyright">
						Copyright  &copy; 2021 - All Rights Reserved   -  Workshop on Autonomous Navigation in Unconstrained Environments (AutoNUE 2021)
					</div>
				</div>
			</section>

		<!-- Scripts -->
			<script src="../assets/js/jquery.min.js"></script>
			<script src="../assets/js/skel.min.js"></script>
			<script src="../assets/js/util.js"></script>
			<script src="../assets/js/main.js"></script>

	</body>

<!-- Mirrored from cvit.iiit.ac.in/autonue2021/challenge/ by HTTrack Website Copier/3.x [XR&CO'2014], Thu, 25 Apr 2024 11:49:41 GMT -->
</html>
