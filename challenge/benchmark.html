<!DOCTYPE HTML>

<html>
	
<!-- Mirrored from cvit.iiit.ac.in/autonue2021/challenge/benchmark.html by HTTrack Website Copier/3.x [XR&CO'2014], Thu, 25 Apr 2024 11:49:41 GMT -->
<head>
        <title>AutoNUE 2021 Challenge</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="../assets/css/main.css" />
		<style>
			.updates{
				background-color: #6f42c1;

			}
		</style>
	</head>
	<body>

		<!-- Header -->
			<header id="header">
				<div class="inner">
					<!-- <a href="index.html" class="logo">AutoNUE 2021</a> -->
					<a href="index-2.html" class="logo"><img src="../images/autonue-logo-cvpr.png" width="280"></a>
					<nav id="nav">
                        <a href="index-2.html">Home</a>
						<a href="overview.html">Overview</a>
						<a href="benchmark.html">Instructions & Benchmark </a>
                        <a href="../index.html" target="_blank">AutoNUE 2021 </a>
                        <a href="faq.html">FAQ</a>
						<a href="contact.html">Contact US</a>
					</nav>
				</div>
			</header>
			<a href="#menu" class="navPanelToggle"><span class="fa fa-bars"></span></a>

	
		<!-- One -->

        <!-- One -->
			<section id="one">
				<div class="inner">
					<header>
						<h2>AutoNUE 2021 Instructions & Benchmark </h2>
                    </header>
<p>
    The challenge will have 5 benchmarks, details of which can be seen below:
    <ol>
        <li>Supervised Domain Adaptation</li>
        <li>Semi-supervised Domain Adaptation</li>
        <li>Weakly supervised Domain Adaptation</li>
        <li>Unsupervised Domain Adaptation</li>
        <li>Semantic Segmentation</li>
    </ol>

  
</p>

<p><strong>Important Note:</strong> We are making the code for Domain Adaptation baselines public <a href="https://github.com/rajp152k/AUTONUE-III-Baselines" > here </a>. If anyone is interested, feel free to use it. For Domain Adaptation Challenges, the participants may be requested to submit the code and <code>requirements.txt</code> (containing all the required installations). </p>


<h3>I. Supervised Domain Adaptation</h3>

<p>
    This challenge involves domain adaptation from around 20k samples of Mapillary, Cityscapes (fine annotations only), Berkeley Deep Drive, and GTA as the source dataset (S) to the IDD as target dataset (T). For the IDD dataset, participants have to submit the results for Level-3 (26 classes) hierarchy.
</p>	

<h4>Directions for Participation</h4>
<ol>
    <li>Register an account at <a href="http://idd.insaan.iiit.ac.in/" rel="nofollow" target="_blank">http://idd.insaan.iiit.ac.in/</a>,
        with the event selected as "AutoNUE Challenge 2021".
    </li>
    <li>Go to Dataset > Download page in the menu.</li>
    <li>Dataset consists of 2 parts which are available for download.</li>
    <li>The first part is the <strong>IDD Segmentation (IDD 20k Part I)</strong>.</li>
    <li>The other part is <strong>IDD Segmentation (IDD 20k Part II)</strong>.</li>
    <li>Extract both the downloaded compressed files into the same folder.</li>
    <li>Please run the data preparation code for generating ground truth segmentation masks as documented
        here: <a href="https://github.com/AutoNUE/public-code" rel="nofollow" target="_blank">https://github.com/AutoNUE/public-code</a>.
        Use the following command for segmentation mask generation: <br>
        <code>python preperation/createLabels.py --datadir $ANUE --id-type level3Id --num-workers $C</code>
    </li>
    <li>Download the target datasets from original websites (all except GTA require registrations), given
        below for easy reference:
        <ol type="a">
            <li><a href="https://www.mapillary.com/dataset/vistas?pKey=q0GhQpk20wJm1ba1mfwJmw"
                   rel="nofollow">https://www.mapillary.com/dataset/vistas?pKey=q0GhQpk20wJm1ba1mfwJmw</a>
            </li>
            <li><a href="https://bdd-data.berkeley.edu/" rel="nofollow">https://bdd-data.berkeley.edu/</a>
                (you
                might have to click on Advanced tab, and then click on "proceed to bdd-data.berkeley.edu")
            </li>
            <li><a href="https://download.visinf.tu-darmstadt.de/data/from_games/" rel="nofollow">https://download.visinf.tu-darmstadt.de/data/from_games/</a>
                (merge all images and labels into a pair folders)
            </li>
            <li><a href="https://www.cityscapes-dataset.com/examples/#fine-annotations" rel="nofollow">https://www.cityscapes-dataset.com/examples/#fine-annotations</a>
                (make sure that you only download dataset with fine annotations)
            </li>
        </ol>
    </li>
    <li>Please run the data preparation code for generating sampled source datasets as documented here:
        <u><a href="https://github.com/AutoNUE/public-code"
              rel="nofollow">https://github.com/AutoNUE/public-code</a></u>.
        Use the following command for segmentation mask generation: <br>
        <code>./domain_adaptation/source/prep_all.sh</code>
        <p>This will create the folder <kbd>public-code/domain_adaptation/source/source_datasets_dir/source_datasets_dir/</kbd> where you will find
        the images and annotations for the source dataset to be used for this challenge.</p>
   </li>
    <li>Use IDD training and validation datasets for the target.</li>
    <li>Once you have built a model, and have the predictions of the model in any of the splits (train,
        val), you can evaluate the metric as directed here: <a
                href="https://github.com/AutoNUE/public-code#evaluation" rel="nofollow">https://github.com/AutoNUE/public-code#evaluation</a>.
        Use the following command for segmentation evaluation: <br>
        <code>python evaluate/evaluate_mIoU.py --gts $GT --preds $PRED --num-workers $C</code> <br>
        Your prediction is a png image, which has the size of 1280x720. Each pixel of this image contains
        the label as level 3ds (see labels code) of the corresponding image (resized to 1280x720). The
        evaluation code above resizes both your prediction and ground truth png files to 1280x720, in case
        they are not of that size.
    </li>
    <li>Finally you can upload the predictions for the test split (4k; 2k each from the two parts of
        IDD20K), to be evaluated for the leaderboard here: <a
                href="http://idd.insaan.iiit.ac.in/evaluation/submission/submit/" rel="nofollow">http://idd.insaan.iiit.ac.in/evaluation/submission/submit/</a>
        <ol type="a">
            <li>Sample format is given in the submission link.</li>
            <li>The suffix of each file should be "_leftImg8bit".</li>
        </ol>
    </li>
</ol>


<h4>Output Format</h4>

<p>
    The output format is a png image with the same resolution as the input image, where the value of every pixel is an integer in {0. .... , 26}, where the first 0-25 classes correspond to the level 3 ids (see Overview, for details of the level 3 ids) and the class 26 is used as a miscellaneous class.
</p>

<h4>Metric</h4>
<p>
    We will be using the mean Intersection over Union metric. All the ground truth and predictions maps will be resized to 720p (using nearest neighbor) and True positives (TP), False Negatives (FN) and False positives (FP) will be computed for each class (except 26) over the entire test split of the dataset. Intersection over Union (IoU) will be computed for each class by the formula TP/(TP+FN+FP) and the mean value is taken as the metric (commonly known as mIoU) for the segmentation challenge.
</p>

<p>
Additionally we will also be reporting the mIoU for level 2 and level 1 ids also at 720p resolution in the leaderboard. Evaluation scripts are available here: <a href="https://github.com/AutoNUE/public-code" rel="nofollow">https://github.com/AutoNUE/public-code</a>

</p>





<h3>II. Semi-supervised Domain Adaptation</h3>
<p>This challenge involves domain adaptation from around 20k samples of Mapillary, Cityscapes (fine annotations only), Berkeley Deep Drive, and GTA as the source dataset (S) to the IDD as target dataset (T). For the IDD dataset, participants have to submit the results for Level-3 (26 classes) hierarchy.</p>
<h4>Directions for Participation</h4>
<ol>
    <li>Register an account at <a href="http://idd.insaan.iiit.ac.in/" rel="nofollow" target="_blank">http://idd.insaan.iiit.ac.in/</a>,
        with the event selected as "AutoNUE Challenge 2021".
    </li>
    <li>Go to Dataset > Download page in the menu.</li>
    <li>Dataset consists of 2 parts which are available for download.</li>
    <li>The first part is the <strong>IDD Segmentation (IDD 20k Part I)</strong>.</li>
    <li>The other part is <strong>IDD Segmentation (IDD 20k Part II)</strong>.</li>
    <li>Extract both the downloaded compressed files into the same folder.</li>
    <li>Please run the data preparation code for generating ground truth segmentation masks as documented
        here: <a href="https://github.com/AutoNUE/public-code" rel="nofollow" target="_blank">https://github.com/AutoNUE/public-code</a>.
        Use the following command for segmentation mask generation: <br>
        <code>python preperation/createLabels.py --datadir $ANUE --id-type level3Id --num-workers $C --semisup_da True</code>
        <p>Note that <em>only selected train masks, which can be used for this challenge, will be generated for the training stage</em>. All validation masks will be generated for the evaluation stage (refer to step 10 below).</p>
    </li>
    <li>Download the target datasets from original websites (all except GTA require registrations), given
        below for easy reference:
        <ol type="a">
            <li><a href="https://www.mapillary.com/dataset/vistas?pKey=q0GhQpk20wJm1ba1mfwJmw"
                   rel="nofollow">https://www.mapillary.com/dataset/vistas?pKey=q0GhQpk20wJm1ba1mfwJmw</a>
            </li>
            <li><a href="https://bdd-data.berkeley.edu/" rel="nofollow">https://bdd-data.berkeley.edu/</a>
                (you might have to click on Advanced tab, and then click on "proceed to bdd-data.berkeley.edu")
            </li>
            <li><a href="https://download.visinf.tu-darmstadt.de/data/from_games/" rel="nofollow">https://download.visinf.tu-darmstadt.de/data/from_games/</a>
                (merge all images and labels into a pair folders)
            </li>
            <li><a href="https://www.cityscapes-dataset.com/examples/#fine-annotations" rel="nofollow">https://www.cityscapes-dataset.com/examples/#fine-annotations</a>
                (make sure that you only download dataset with fine annotations)
            </li>
        </ol>
    </li>
    <li>Please run the data preparation code for generating sampled source datasets as documented here:
        <u><a href="https://github.com/AutoNUE/public-code"
              rel="nofollow">https://github.com/AutoNUE/public-code</a></u>.
        Use the following command for segmentation mask generation: <br>
        <code>./domain_adaptation/source/prep_all.sh</code>
        <p>This will create the folder <kbd>public-code/domain_adaptation/source/source_datasets_dir/source_datasets_dir/</kbd> where you will find
        the images and annotations for the source dataset to be used for this challenge.</p>
   </li>
    <li>Once you have built a model, and have the predictions of the model in any of the splits (train,
        val), you can evaluate the metric as directed here: <a
                href="https://github.com/AutoNUE/public-code#evaluation" rel="nofollow">https://github.com/AutoNUE/public-code#evaluation</a>.
        Use the following command for segmentation evaluation: <br>
        <code>python evaluate/evaluate_mIoU.py --gts $GT --preds $PRED --num-workers $C</code> <br>
        Your prediction is a png image, which has the size of 1280x720. Each pixel of this image contains
        the label as level 3ds (see labels code) of the corresponding image (resized to 1280x720). The
        evaluation code above resizes both your prediction and ground truth png files to 1280x720, in case
        they are not of that size.
    </li>
    <li>Finally you can upload the predictions for the test split (4k; 2k each from the two parts of
        IDD20K), to be evaluated for the leaderboard here: <a href="http://idd.insaan.iiit.ac.in/evaluation/submission/submit/" rel="nofollow">http://idd.insaan.iiit.ac.in/evaluation/submission/submit/</a>
        <ol type="a">
            <li>Sample format is given in the submission link.</li>
            <li>The suffix of each file should be "_leftImg8bit".</li>
        </ol>
    </li>
</ol>

<h4>Output Format</h4>
<p>
    The output format is a png image with the same resolution as the input image, where the value of every pixel is an integer in {0. .... , 26}, where the first 0-25 classes correspond to the level 3 ids (see Overview, for details of the level 3 ids) and the class 26 is used as a miscellaneous class.
</p>

<h4>Metric</h4>
<p>
    We will be using the mean Intersection over Union metric. All the ground truth and predictions maps will be resized to 720p (using nearest neighbor) and True positives (TP), False Negatives (FN) and False positives (FP) will be computed for each class (except 26) over the entire test split of the dataset. Intersection over Union (IoU) will be computed for each class by the formula TP/(TP+FN+FP) and the mean value is taken as the metric (commonly known as mIoU) for the segmentation challenge.
</p>

<p>Additionally we will also be reporting the mIoU for level 2 and level 1 ids also at 720p resolution in the leaderboard. Evaluation scripts are available here: <a href="https://github.com/AutoNUE/public-code" rel="nofollow">https://github.com/AutoNUE/public-code</a></p>
        




<h3>III. Weakly-supervised Domain Adaptation</h3>
<p>This challenge involves domain adaptation from around 20k samples of Mapillary, Cityscapes (fine annotations only), Berkeley Deep Drive, and GTA as the source dataset (S) to the IDD as target dataset (T). For the IDD dataset, participants have to submit the results for Level-3 (26 classes) hierarchy.</p>
<h4>Directions for Participation</h4>
<ol>
    <li>Register an account at <a href="http://idd.insaan.iiit.ac.in/" rel="nofollow" target="_blank">http://idd.insaan.iiit.ac.in/</a>,
        with the event selected as "AutoNUE Challenge 2021".
    </li>
    <li>Go to Dataset > Download page in the menu.</li>
    <li>Dataset consists of 2 parts which are available for download.</li>
    <li>The first part is the <strong>IDD Segmentation (IDD 20k Part I)</strong>.</li>
    <li>The other part is <strong>IDD Segmentation (IDD 20k Part II)</strong>.</li>
    <li>Extract both the downloaded compressed files into the same folder.</li>
    <li>Please run the data preparation code for generating ground truth segmentation masks as documented
        here: <a href="https://github.com/AutoNUE/public-code" rel="nofollow" target="_blank">https://github.com/AutoNUE/public-code</a>.
        Use the following command for segmentation mask generation: <br>
        <code>python preperation/createLabels.py --datadir $ANUE --id-type level3Id --num-workers $C --weaksup_da True</code>
        <p>Note that only validation masks will be generated for this challenge towards the evaluation stage (refer to step 11 below). <em>Bounding box annotations to be used for training in this challenge are present here:</em> <a href="https://github.com/AutoNUE/public-code/target/weakly-supervised" rel="nofollow">https://github.com/AutoNUE/public-code/target/weakly-supervised</a></p>
    </li>
    <li>Download the target datasets from original websites (all except GTA require registrations), given
        below for easy reference:
        <ol type="a">
            <li><a href="https://www.mapillary.com/dataset/vistas?pKey=q0GhQpk20wJm1ba1mfwJmw"
                   rel="nofollow">https://www.mapillary.com/dataset/vistas?pKey=q0GhQpk20wJm1ba1mfwJmw</a>
            </li>
            <li><a href="https://bdd-data.berkeley.edu/" rel="nofollow">https://bdd-data.berkeley.edu/</a>
                (you might have to click on Advanced tab, and then click on "proceed to bdd-data.berkeley.edu")
            </li>
            <li><a href="https://download.visinf.tu-darmstadt.de/data/from_games/" rel="nofollow">https://download.visinf.tu-darmstadt.de/data/from_games/</a>
                (merge all images and labels into a pair folders)
            </li>
            <li><a href="https://www.cityscapes-dataset.com/examples/#fine-annotations" rel="nofollow">https://www.cityscapes-dataset.com/examples/#fine-annotations</a>
                (make sure that you only download dataset with fine annotations)
            </li>
        </ol>
    </li>
    <li>Please run the data preparation code for generating sampled source datasets as documented here:
        <u><a href="https://github.com/AutoNUE/public-code"
              rel="nofollow">https://github.com/AutoNUE/public-code</a></u>.
        Use the following command for segmentation mask generation: <br>
        <code>./domain_adaptation/source/prep_all.sh</code>
        <p>This will create the folder <kbd>public-code/domain_adaptation/source/source_datasets_dir/source_datasets_dir/</kbd> where you will find
        the images and annotations for the source dataset to be used for this challenge.</p>
   </li>
    <li>Once you have built a model, and have the predictions of the model in any of the splits (train,
        val), you can evaluate the metric as directed here: <a
                href="https://github.com/AutoNUE/public-code#evaluation" rel="nofollow">https://github.com/AutoNUE/public-code#evaluation</a>.
        Use the following command for segmentation evaluation: <br>
        <code>python evaluate/evaluate_mIoU.py --gts $GT --preds $PRED --num-workers $C</code> <br>
        Your prediction is a png image, which has the size of 1280x720. Each pixel of this image contains
        the label as level 3ds (see labels code) of the corresponding image (resized to 1280x720). The
        evaluation code above resizes both your prediction and ground truth png files to 1280x720, in case
        they are not of that size.
    </li>
    <li>Finally you can upload the predictions for the test split (4k; 2k each from the two parts of
        IDD20K), to be evaluated for the leaderboard here: <a href="http://idd.insaan.iiit.ac.in/evaluation/submission/submit/" rel="nofollow">http://idd.insaan.iiit.ac.in/evaluation/submission/submit/</a>
        <ol type="a">
            <li>Sample format is given in the submission link.</li>
            <li>The suffix of each file should be "_leftImg8bit".</li>
        </ol>
    </li>
</ol>
<h4>Output Format</h4>
<p>The output format is a png image with the same resolution as the input image, where the value of every pixel is an integer in {0. .... , 26}, where the first 0-25 classes correspond to the level 3 ids (see Overview, for details of the level 3 ids) and the class 26 is used as a miscellaneous class.</p>

<h4>Metric</h4>
<p>We will be using the mean Intersection over Union metric. All the ground truth and predictions maps will be resized to 720p (using nearest neighbor) and True positives (TP), False Negatives (FN) and False positives (FP) will be computed for each class (except 26) over the entire test split of the dataset. Intersection over Union (IoU) will be computed for each class by the formula TP/(TP+FN+FP) and the mean value is taken as the metric (commonly known as mIoU) for the segmentation challenge.</p>
<p>Additionally we will also be reporting the mIoU for level 2 and level 1 ids also at 720p resolution in the leaderboard. Evaluation scripts are available here: <a href="https://github.com/AutoNUE/public-code" rel="nofollow">https://github.com/AutoNUE/public-code</a></p>






<h3>IV. Unsupervised Domain Adaptation</h3>
    <p>This challenge involves domain adaptation from around 20k samples of Mapillary, Cityscapes (fine annotations only), Berkeley Deep Drive, and GTA as the source dataset (S) to the IDD as target dataset (T). For the IDD dataset, participants have to submit the results for Level-3 (26 classes) hierarchy.</p>
<h4>Directions for Participation</h4>
    <ol>
        <li>Register an account at <a href="http://idd.insaan.iiit.ac.in/" rel="nofollow" target="_blank">http://idd.insaan.iiit.ac.in/</a>,
            with the event selected as "AutoNUE Challenge 2021".
        </li>
        <li>Go to Dataset > Download page in the menu.</li>
        <li>Dataset consists of 2 parts which are available for download.</li>
        <li>The first part is the <strong>IDD Segmentation (IDD 20k Part I)</strong>.</li>
        <li>The other part is <strong>IDD Segmentation (IDD 20k Part II)</strong>.</li>
        <li>Extract both the downloaded compressed files into the same folder.</li>
        <li>Please run the data preparation code for generating ground truth segmentation masks as documented
            here: <a href="https://github.com/AutoNUE/public-code" rel="nofollow" target="_blank">https://github.com/AutoNUE/public-code</a>.
            Use the following command for segmentation mask generation: <br>
            <code>python preperation/createLabels.py --datadir $ANUE --id-type level3Id --num-workers $C  --unsup_da True</code>
            <p>Note that only validation masks will be generated for this challenge towards the evaluation stage (refer to step 11 below). <u>IDD Training labels cannot be used for this challenge. Images from training data can be used.</u></p>
        </li>
        <li>Download the target datasets from original websites (all except GTA require registrations), given
            below for easy reference:
            <ol type="a">
                <li><a href="https://www.mapillary.com/dataset/vistas?pKey=q0GhQpk20wJm1ba1mfwJmw"
                       rel="nofollow">https://www.mapillary.com/dataset/vistas?pKey=q0GhQpk20wJm1ba1mfwJmw</a>
                </li>
                <li><a href="https://bdd-data.berkeley.edu/" rel="nofollow">https://bdd-data.berkeley.edu/</a>
                    (you might have to click on Advanced tab, and then click on "proceed to bdd-data.berkeley.edu")
                </li>
                <li><a href="https://download.visinf.tu-darmstadt.de/data/from_games/" rel="nofollow">https://download.visinf.tu-darmstadt.de/data/from_games/</a>
                    (merge all images and labels into a pair folders)
                </li>
                <li><a href="https://www.cityscapes-dataset.com/examples/#fine-annotations" rel="nofollow">https://www.cityscapes-dataset.com/examples/#fine-annotations</a>
                    (make sure that you only download dataset with fine annotations)
                </li>
            </ol>
        </li>
        <li>Please run the data preparation code for generating sampled source datasets as documented here:
            <u><a href="https://github.com/AutoNUE/public-code"
                  rel="nofollow">https://github.com/AutoNUE/public-code</a></u>.
            Use the following command for segmentation mask generation: <br>
            <code>./domain_adaptation/source/prep_all.sh</code>
            <p>This will create the folder <kbd>public-code/domain_adaptation/source/source_datasets_dir/source_datasets_dir/</kbd> where you will find
            the images and annotations for the source dataset to be used for this challenge.</p>
       </li>
        <li>Once you have built a model, and have the predictions of the model in any of the splits (train,
            val), you can evaluate the metric as directed here: <a
                    href="https://github.com/AutoNUE/public-code#evaluation" rel="nofollow">https://github.com/AutoNUE/public-code#evaluation</a>.
            Use the following command for segmentation evaluation: <br>
            <code>python evaluate/evaluate_mIoU.py --gts $GT --preds $PRED --num-workers $C</code> <br>
            Your prediction is a png image, which has the size of 1280x720. Each pixel of this image contains
            the label as level 3ds (see labels code) of the corresponding image (resized to 1280x720). The
            evaluation code above resizes both your prediction and ground truth png files to 1280x720, in case
            they are not of that size.
        </li>
        <li>Finally you can upload the predictions for the test split (4k; 2k each from the two parts of
            IDD20K), to be evaluated for the leaderboard here: <a href="http://idd.insaan.iiit.ac.in/evaluation/submission/submit/" rel="nofollow">http://idd.insaan.iiit.ac.in/evaluation/submission/submit/</a>
            <ol type="a">
                <li>Sample format is given in the submission link.</li>
                <li>The suffix of each file should be "_leftImg8bit".</li>
            </ol>
        </li>
    </ol>

<h4>Output Format</h4>
    <p>The output format is a png image with the same resolution as the input image, where the value of every pixel is an integer in {0. .... , 26}, where the first 0-25 classes correspond to the level 3 ids (see Overview, for details of the level 3 ids) and the class 26 is used as a miscellaneous class.</p>
<h4>Metric</h4>
    <p>We will be using the mean Intersection over Union metric. All the ground truth and predictions maps will be resized to 720p (using nearest neighbor) and True positives (TP), False Negatives (FN) and False positives (FP) will be computed for each class (except 26) over the entire test split of the dataset. Intersection over Union (IoU) will be computed for each class by the formula TP/(TP+FN+FP) and the mean value is taken as the metric (commonly known as mIoU) for the segmentation challenge.</p>
    <p>Additionally we will also be reporting the mIoU for level 2 and level 1 ids also at 720p resolution in the leaderboard. Evaluation scripts are available here: <a href="https://github.com/AutoNUE/public-code" rel="nofollow">https://github.com/AutoNUE/public-code</a></p>

<h3>V. Semantic Segmentation</h3>
    <p>The segmentation challenge involves pixel level predictions for all the 26 classes at level 3 of the label hierarchy (see Overview, for details of the level 3 ids).</p>

<h4>Directions for Participation</h4>
    <ol>
        <li>Register an account at <a href="http://idd.insaan.iiit.ac.in/" rel="nofollow" target="_blank">http://idd.insaan.iiit.ac.in/</a>,
            with the event selected as "AutoNUE Challenge 2021".
        </li>
        <li>Go to Dataset > Download page in the menu.</li>
        <li>Dataset consists of 2 parts which are available for download.</li>
        <li>The first part is the <strong>IDD Segmentation (IDD 20k Part I)</strong>.</li>
        <li>The other part is <strong>IDD Segmentation (IDD 20k Part II)</strong>.</li>
        <li>Extract both the downloaded compressed files into the same folder.</li>
        <li>Please run the data preparation code for generating ground truth segmentation masks as documented
            here: <a href="https://github.com/AutoNUE/public-code" rel="nofollow" target="_blank">https://github.com/AutoNUE/public-code</a>.
            Use the following command for segmentation mask generation: <br>
            <code>python preperation/createLabels.py --datadir $ANUE --id-type level3Id --num-workers $C</code>
        </li>
        <li>Once you have built a model, and have the predictions of the model in any of the splits (train,
            val), you can evaluate the metric as directed here: <a
                    href="https://github.com/AutoNUE/public-code#evaluation" rel="nofollow">https://github.com/AutoNUE/public-code#evaluation</a>.
            Use the following command for segmentation evaluation: <br>
            <code>python evaluate/evaluate_mIoU.py --gts $GT --preds $PRED --num-workers $C</code> <br>
            Your prediction is a png image, which has the size of 1280x720. Each pixel of this image contains
            the label as level 3ds (see labels code) of the corresponding image (resized to 1280x720). The
            evaluation code above resizes both your prediction and ground truth png files to 1280x720, in case
            they are not of that size.
        </li>
        <li>Finally you can upload the predictions for the test split (4k; 2k each from the two parts of
            IDD20K), to be evaluated for the leaderboard here: <a href="http://idd.insaan.iiit.ac.in/evaluation/submission/submit/" rel="nofollow">http://idd.insaan.iiit.ac.in/evaluation/submission/submit/</a>
            <ol type="a">
                <li>Sample format is given in the submission link.</li>
                <li>The suffix of each file should be "_leftImg8bit".</li>
            </ol>
        </li>
    </ol>

<h4>Output Format</h4>
    <p>The output format is a png image with the same resolution as the input image, where the value of every pixel is an integer in {0. .... , 26}, where the first 0-25 classes correspond to the level 3 ids (see Overview, for details of the level 3 ids) and the class 26 is used as a miscellaneous class.</p>
<h4>Metric</h4>
    <p>We will be using the mean Intersection over Union metric. All the ground truth and predictions maps will be resized to 720p (using nearest neighbor) and True positives (TP), False Negatives (FN) and False positives (FP) will be computed for each class (except 26) over the entire test split of the dataset. Intersection over Union (IoU) will be computed for each class by the formula TP/(TP+FN+FP) and the mean value is taken as the metric (commonly known as mIoU) for the segmentation challenge.</p>
    <p>Additionally we will also be reporting the mIoU for level 2 and level 1 ids also at 720p resolution in the leaderboard. Evaluation scripts are available here: <a href="https://github.com/AutoNUE/public-code" rel="nofollow">https://github.com/AutoNUE/public-code</a></p>

				</div>
			</section>


	

		<!-- Footer -->
			<section id="footer">
				<div class="inner">
							
					<div class="copyright">
						Copyright  &copy; 2021 - All Rights Reserved   -  Workshop on Autonomous Navigation in Unconstrained Environments (AutoNUE 2021)
					</div>
				</div>
			</section>

		<!-- Scripts -->
			<script src="../assets/js/jquery.min.js"></script>
			<script src="../assets/js/skel.min.js"></script>
			<script src="../assets/js/util.js"></script>
			<script src="../assets/js/main.js"></script>

	</body>

<!-- Mirrored from cvit.iiit.ac.in/autonue2021/challenge/benchmark.html by HTTrack Website Copier/3.x [XR&CO'2014], Thu, 25 Apr 2024 11:49:41 GMT -->
</html>
